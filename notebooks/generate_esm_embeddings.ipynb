{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPv46-BqrQy9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title 1. installation\n",
        "#@markdown\n",
        "!pip install biopython\n",
        "!pip install fair-esm\n",
        "from Bio import SeqIO\n",
        "import torch\n",
        "import esm\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from google.colab import files\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Evolutionary Scale Modeling (esm)](https://github.com/facebookresearch/esm?tab=readme-ov-file#pre-trained-models-) is a set of protein language models with varying numbers of layers, parameters, size (in GB), etc. Only those models that fit within the 16GB vRAM of the Tesla T4 GPU are listed in the following table. Choose a model based on your requirements. Bigger model tend to perfomr well, but are slow to compute.\n",
        "\n",
        "| model |  layers | parameters |  size (GB)|\n",
        "|------------------------------|----|----------|---|\n",
        "| `esm2_t36_3B_UR50D`          | 36 | 3B      | 5.3|\n",
        "| `esm2_t33_650M_UR50D`        | 33 | 650M    | 2.4|\n",
        "| `esm2_t30_150M_UR50D`        | 30 | 150M    | 0.56|\n",
        "| `esm2_t12_35M_UR50D`         | 12 | 35M     | 0.12|\n",
        "| `esm2_t6_8M_UR50D`           | 6  | 8M      | 0.02|\n",
        "| `esm1b_t33_650M_UR50S`      | 33  | 650M      | 7.3|\n",
        "| `esm1_t34_670M_UR50S`      | 34 | 670M      | 6.2|\n",
        "| `esm1_t34_670M_UR50D`         | 34  | 670M      | 6.2|\n",
        "| `esm1_t12_85M_UR50S`         | 12  | 85M      | 0.97|\n",
        "| `esm1_t6_43M_UR50S`         | 6  | 43M      | 0.48|\n"
      ],
      "metadata": {
        "id": "tTAjHOYFkKLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#@title Choose a model\n",
        "#@markdown\n",
        "\n",
        "# Load ESM-2 model\n",
        "protein_language_model  =  \"esm2_t33_650M_UR50D\" #@param [\"esm2_t36_3B_UR50D\",\"esm2_t33_650M_UR50D\",\"esm2_t30_150M_UR50D\",\"esm2_t12_35M_UR50D\",\"esm2_t6_8M_UR50D\",\"esm1b_t33_650M_UR50S\",\"esm1_t34_670M_UR50S\",\"esm1_t34_670M_UR50D\",\"esm1_t12_85M_UR50S\",\"esm1_t6_43M_UR50S\"]\n",
        "\n",
        "if protein_language_model == \"esm2_t36_3B_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm2_t36_3B_UR50D()\n",
        "elif protein_language_model == \"esm2_t33_650M_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "elif protein_language_model == \"esm2_t30_150M_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
        "elif protein_language_model == \"esm2_t12_35M_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "elif protein_language_model == \"esm2_t6_8M_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "elif protein_language_model == \"esm1b_t33_650M_UR50S\":\n",
        "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "elif protein_language_model == \"esm1_t34_670M_UR50S\":\n",
        "    model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
        "elif protein_language_model == \"esm1_t34_670M_UR50D\":\n",
        "    model, alphabet = esm.pretrained.esm1_t34_670M_UR50D()\n",
        "elif protein_language_model == \"esm1_t12_85M_UR50S\":\n",
        "    model, alphabet = esm.pretrained.esm1_t12_85M_UR50S()\n",
        "elif protein_language_model == \"esm1_t6_43M_UR50S\":\n",
        "    model, alphabet = esm.pretrained.esm1_t6_43M_UR50S()\n",
        "\n",
        "model = model.cuda()  # Mover el modelo a la GPU\n",
        "batch_converter = alphabet.get_batch_converter()"
      ],
      "metadata": {
        "id": "eJAzYWwWrW0Y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#@title ##Upload your sequences (fasta format)\n",
        "\n",
        "# rename the sequences\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "os.rename(file_name, 'seqs.fasta')\n",
        "fasta_file = \"seqs.fasta\"\n",
        "sequences = [(seq_record.id, str(seq_record.seq)) for seq_record in SeqIO.parse(fasta_file, \"fasta\")]"
      ],
      "metadata": {
        "id": "gjYOofOHrW23",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#@title ## Generate embeddings\n",
        "# Calcular el número de lotes de 100 secuencias\n",
        "num_sequences = len(sequences)\n",
        "num_batches = math.ceil(num_sequences / 100)\n",
        "\n",
        "# Crear una barra de progreso\n",
        "pbar = tqdm(total=num_batches, desc=\"Generating embeddings ...\")\n",
        "\n",
        "# Lista para almacenar todas las representaciones de secuencias\n",
        "all_sequence_representations = []\n",
        "\n",
        "# Procesar cada lote de 100 secuencias\n",
        "for i in range(0, num_sequences, 100):\n",
        "    # Obtener el lote de secuencias\n",
        "    batch_data = x[i:i+100]\n",
        "\n",
        "    # Preparar los datos para el modelo\n",
        "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
        "    batch_tokens = batch_tokens.cuda()\n",
        "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "    num_layers = {\n",
        "    \"esm2_t36_3B_UR50D\": 36,\n",
        "    \"esm2_t33_650M_UR50D\": 33,\n",
        "    \"esm2_t30_150M_UR50D\": 30,\n",
        "    \"esm2_t12_35M_UR50D\": 12,\n",
        "    \"esm2_t6_8M_UR50D\": 6,\n",
        "    \"esm1b_t33_650M_UR50S\": 33,\n",
        "    \"esm1_t34_670M_UR50S\": 34,\n",
        "    \"esm1_t34_670M_UR50D\": 34,\n",
        "    \"esm1_t12_85M_UR50S\": 12,\n",
        "    \"esm1_t6_43M_UR50S\": 6}\n",
        "\n",
        "  # Seleccionar el número de capas basado en el modelo seleccionado\n",
        "    layer = num_layers.get(protein_language_model, None)\n",
        "    with torch.no_grad():\n",
        "      results = model(batch_tokens, repr_layers=[layer], return_contacts=False)\n",
        "    token_representations = results[\"representations\"][layer]\n",
        "\n",
        "    ############\n",
        "    # Extraer representaciones por token\n",
        "    #with torch.no_grad():\n",
        "    #    results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "    #token_representations = results[\"representations\"][33]\n",
        "    ###################\n",
        "\n",
        "    # Generar representaciones por secuencia mediante promedio\n",
        "    sequence_representations = []\n",
        "    for j, tokens_len in enumerate(batch_lens):\n",
        "        sequence_representations.append(token_representations[j, 1:tokens_len-1].mean(0))\n",
        "\n",
        "    # Actualizar la lista de representaciones de secuencias\n",
        "    all_sequence_representations.extend(sequence_representations)\n",
        "\n",
        "    # Actualizar la barra de progreso\n",
        "    pbar.update(1)\n",
        "\n",
        "# Cerrar la barra de progreso\n",
        "pbar.close()"
      ],
      "metadata": {
        "id": "faLvNpL-raSA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Download results\n",
        "df = pd.DataFrame(sequences, columns=['protein', 'sequence'])\n",
        "df[\"embeddings\"] = [embedding.cpu().numpy() for embedding in all_sequence_representations]\n",
        "df.to_pickle(\"embeddings.pkl\")\n",
        "files.download('embeddings.pkl')"
      ],
      "metadata": {
        "id": "LwI478MzrcfY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}